Components to build:
    
1. Environment -> State representation
    -Trivially: BoardState() OR BoardState.Field (object also contains Lost, BOARD_SIZE, and move history; move history being the largest issue here)
2. Action scoring algorithm: f(State[n], State[n-1]) -> Reward
    -Trivially: Score
3. Agent Policy Builder
    -Discounting algorithm for rewards applied to previous moves
        *Discount factor in the form: f(MoveCount, CurrentMove, HPDiscountFactor) -> Discount
        *Hyper-parameter: Discount Factor
        *Trivially: Exponential decay        
        *Structured to easily swap out different mechanisms
    -Randomness algorithm/annealing (likelihood of using a random move instead of recommended move)
        *Level of randomness in the form: f(IterationCount, CurrentIteration, HPRandomness) -> Randomness
        *Hyper-parameter: Randomness
        *Trivially: Exponential decay
            > Option 1: Random score added to random choice on every move
            > Option 2: Random choice overrides scoring on a subset of moves
        *Structured to easily swap out different mechanisms
    -In the form: f(IterationCount, Algorithms, HPs)->Policy
        *HPs are dict<float>
        *Algorithms are dict<method>
        *Policy fields:
            > State
            > Action weights/expected outcome
        *The policy CANNOT contain every board state (even allowing only up to 2048, this would be 185 quadrillion entries)
            > How many states are there if rotated boards are considered the same as well as boards that are divided by 2 until the lowest value on the board is a 2?
            > What is an efficient algorithm to rotate a board so it will always match other boards that are identical?
        *What format should the policy be in? Tentatively, either text or SQLITE. Ideally the policy should be small enough to be held in memory, then serialized into a file.
        *Multithreading: Batch iterations so that parameters for scalar algorithms (randomness and discount) can be adjusted at a given resolution (i.e. every 1000 iterations)
4. Agent
    -Ability to choose policy (probably based on filename)
    -Uses policy to play
    -Will need interface to provide single move weights




